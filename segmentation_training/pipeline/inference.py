from typing import Optional, Callable

import numpy as np
import torch
from slicing_utils.tiles import ImageSlicer
from data.preprocessing import get_val_transform, get_final_transform


def infer(
    model: torch.nn.Module,
    image: np.ndarray,
    device: str | None | torch.device = None,
    transform: Optional[Callable] = None,
    final_transform: Optional[Callable] = None,
):
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    if transform is not None:
        image = transform(image=image)["image"]
    if final_transform is not None:
        image_tensor = final_transform(image=image)["image"]
    image_tensor = image_tensor.to(device).unsqueeze(0)

    with torch.no_grad():
        mask = model(image_tensor).detach().cpu()
        mask = mask[0, 0]
        mask = mask.numpy()

    return mask


def sliced_inference(
    model: torch.nn.Module, image: np.ndarray, slicer: ImageSlicer, conf: Optional[float] = 0.5,
) -> np.ndarray:
    """
    Run sliced inference on an image.
    Output format:
        a binary mask with values 0 or 255

    If conf is None, a non-thresholded confidence map is returned
    """
    image_h, image_w, _ = image.shape

    assert image_h == slicer.image_height and image_w == slicer.image_width, (
        f"Image dimensions ({image_h}, {image_w}) do not match the slicer's expected dimensions "
        f"({slicer.image_height}, {slicer.image_width})."
    )

    # Initialize confidence and scale maps
    confidence_map = np.zeros((image_h, image_w), dtype=np.float32)
    scale_map = np.zeros((image_h, image_w))

    transform = get_val_transform(slicer.tile_size)
    final_transform = get_final_transform()

    # Iterate through tiles generated by the slicer
    for tile, (x, y, tile_width, tile_height) in slicer.iter_split(image):
        mask = infer(model, tile, transform=transform, final_transform=final_transform)

        x1, y1 = x, y
        x2, y2 = x1 + tile_width, y1 + tile_height
        x1_conf, y1_conf = max(x1, 0), max(y1, 0)
        x2_conf, y2_conf = min(x2, image_w), min(y2, image_h)

        confidence_map[y1_conf:y2_conf, x1_conf:x2_conf] += mask[
            y1_conf - y1 : y2_conf - y1, x1_conf - x1 : x2_conf - x1
        ]
        scale_map[y1_conf:y2_conf, x1_conf:x2_conf] += 1

    # Ensure every pixel has been processed
    assert np.all(scale_map != 0), "Part of the image is missing!"

    # Normalize the confidence map and create the final binary mask
    confidence_map /= scale_map
    if conf is None:
        return confidence_map

    pred_mask = (confidence_map > conf).astype(np.uint8) * 255

    return pred_mask
